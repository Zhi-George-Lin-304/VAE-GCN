{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4538c5ce-3d0d-4fad-91ca-51b7d7787cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import rdmolops\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Function to generate the adjacency matrix with diagonal = 1\n",
    "def generate_adjacency_matrix(mol):\n",
    "    num_atoms = mol.GetNumAtoms()\n",
    "    adj_matrix = np.zeros((num_atoms, num_atoms), dtype=np.float32)\n",
    "\n",
    "    for bond in mol.GetBonds():\n",
    "        i = bond.GetBeginAtomIdx()\n",
    "        j = bond.GetEndAtomIdx()\n",
    "        bond_type = bond.GetBondType()\n",
    "\n",
    "        # Map bond types to numeric values\n",
    "        if bond_type == Chem.rdchem.BondType.SINGLE:\n",
    "            adj_matrix[i, j] = 1.0\n",
    "            adj_matrix[j, i] = 1.0\n",
    "        elif bond_type == Chem.rdchem.BondType.DOUBLE:\n",
    "            adj_matrix[i, j] = 2.0\n",
    "            adj_matrix[j, i] = 2.0\n",
    "        elif bond_type == Chem.rdchem.BondType.AROMATIC:\n",
    "            adj_matrix[i, j] = 1.5\n",
    "            adj_matrix[j, i] = 1.5\n",
    "        elif bond_type == Chem.rdchem.BondType.TRIPLE:\n",
    "            adj_matrix[i, j] = 3.0\n",
    "            adj_matrix[j, i] = 3.0\n",
    "\n",
    "    np.fill_diagonal(adj_matrix, 1.0)  # Add self-loops\n",
    "    return adj_matrix\n",
    "\n",
    "# Function to generate the feature matrix (61) \n",
    "def generate_feature_matrix(mol):\n",
    "    import torch\n",
    "    from rdkit import Chem\n",
    "\n",
    "    atom_types = ['As', 'B', 'Br', 'C', 'Cl', 'F', 'I', 'N', 'O', 'P', 'S', 'Se', 'Si']\n",
    "    feature_matrix = []\n",
    "\n",
    "    valence_electrons = {\n",
    "        'As': 5, 'B': 3, 'Br': 7, 'C': 4, 'Cl': 7, 'F': 7, 'I': 7,\n",
    "        'N': 5, 'O': 6, 'P': 5, 'S': 6, 'Se': 6, 'Si': 4\n",
    "    }\n",
    "\n",
    "    for atom in mol.GetAtoms():\n",
    "        features = []\n",
    "\n",
    "        # Atom type (13)\n",
    "        atom_symbol = atom.GetSymbol()\n",
    "        features.extend([1 if atom_symbol == t else 0 for t in atom_types])\n",
    "\n",
    "        # Number of implicit hydrogens (max: 4)\n",
    "        num_hydrogens = atom.GetTotalNumHs()\n",
    "        features.extend([1 if num_hydrogens == i else 0 for i in range(5)])\n",
    "\n",
    "        # Aromaticity (1)\n",
    "        features.append(1 if atom.GetIsAromatic() else 0)\n",
    "\n",
    "        # Hybridization (3)\n",
    "        hybridization = atom.GetHybridization()\n",
    "        features.extend([\n",
    "            1 if hybridization == Chem.rdchem.HybridizationType.SP else 0,\n",
    "            1 if hybridization == Chem.rdchem.HybridizationType.SP2 else 0,\n",
    "            1 if hybridization == Chem.rdchem.HybridizationType.SP3 else 0\n",
    "        ])\n",
    "\n",
    "        # Formal charge (considered -3 to +3)\n",
    "        formal_charge = atom.GetFormalCharge()\n",
    "        features.extend([1 if formal_charge == i else 0 for i in range(-3, 4)])\n",
    "\n",
    "        # Valence electrons (numerical, one-hot encoding for 1-8)\n",
    "        valence = valence_electrons.get(atom_symbol, 0)\n",
    "        features.extend([1 if valence == i else 0 for i in range(1, 9)])\n",
    "\n",
    "        # Atom degree (one-hot encoding for 0-4)\n",
    "        degree = atom.GetDegree()\n",
    "        features.extend([1 if degree == i else 0 for i in range(5)])\n",
    "\n",
    "        # Implicit valence (one-hot encoding for 0-8)\n",
    "        implicit_valence = atom.GetImplicitValence()\n",
    "        features.extend([1 if implicit_valence == i else 0 for i in range(9)])\n",
    "\n",
    "        # Chirality (binary)\n",
    "        features.append(1 if atom.HasProp('_CIPCode') else 0)\n",
    "\n",
    "        # Is heteroatom (binary)\n",
    "        features.append(1 if atom.GetAtomicNum() not in [1, 6] else 0)\n",
    "\n",
    "        # Is in ring (binary)\n",
    "        features.append(1 if atom.IsInRing() else 0)\n",
    "\n",
    "        # Total number of bonds (one-hot encoding for 1-6)\n",
    "        num_bonds = len(atom.GetBonds())\n",
    "        features.extend([1 if num_bonds == i else 0 for i in range(1, 7)])\n",
    "\n",
    "        # Is terminal atom (binary)\n",
    "        features.append(1 if atom.GetDegree() == 1 else 0)\n",
    "\n",
    "        feature_matrix.append(features)\n",
    "\n",
    "    return torch.tensor(feature_matrix, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# MolecularDataset class       \n",
    "class MolecularDataset(Dataset):\n",
    "    def __init__(self, csv_file, target_col, max_atoms=460):\n",
    "        data = pd.read_csv(csv_file)\n",
    "        \n",
    "        # Drop rows where SMILES is NaN or invalid\n",
    "        data = data.dropna(subset=['SMILES'])\n",
    "        data = data[data['SMILES'].apply(lambda x: isinstance(x, str))]\n",
    "        \n",
    "        self.smiles = []\n",
    "        self.targets = []\n",
    "        self.max_atoms = max_atoms  # Fixed maximum number of atoms\n",
    "        \n",
    "        for i, row in data.iterrows():\n",
    "            smiles = row['SMILES']\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if mol:  # Only keep valid SMILES\n",
    "                self.smiles.append(smiles)\n",
    "                self.targets.append(row[target_col])\n",
    "        \n",
    "        self.targets = torch.tensor(self.targets, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.smiles)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        smiles = self.smiles[idx]\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "  \n",
    "        adj = generate_adjacency_matrix(mol)\n",
    "        features = generate_feature_matrix(mol)\n",
    "        \n",
    "        # Pad adjacency matrix to (max_atoms, max_atoms)\n",
    "        padded_adj = np.zeros((self.max_atoms, self.max_atoms), dtype=np.float32)\n",
    "        num_atoms = adj.shape[0]\n",
    "        padded_adj[:num_atoms, :num_atoms] = adj\n",
    "  \n",
    "        # Add self-loops and normalize\n",
    "        adj_hat = padded_adj + np.eye(self.max_atoms, dtype=np.float32)\n",
    "        D_hat_inv_sqrt = np.diag(np.power(np.sum(adj_hat, axis=1), -0.5, where=np.sum(adj_hat, axis=1) != 0))\n",
    "        adj_normalized = np.matmul(np.matmul(D_hat_inv_sqrt, adj_hat), D_hat_inv_sqrt)\n",
    "  \n",
    "        # Pad feature matrix to (max_atoms, feature_dim)\n",
    "        padded_features = np.zeros((self.max_atoms, 61), dtype=np.float32)\n",
    "        padded_features[:num_atoms, :] = features\n",
    "  \n",
    "        target = self.targets[idx]\n",
    "        return (\n",
    "          torch.tensor(adj_normalized, dtype=torch.float32).to(device),\n",
    "          torch.tensor(padded_features, dtype=torch.float32).to(device),\n",
    "          target.to(device),\n",
    "          torch.tensor(num_atoms, dtype=torch.int64).to(device) \n",
    "          )\n",
    "\n",
    "# GCN Layer (ReLU)\n",
    "#class GCNLayer(nn.Module):\n",
    "   # def __init__(self, input_dim, output_dim):\n",
    "   #     super(GCNLayer, self).__init__()\n",
    "   #     self.linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "   # def forward(self, A_normalized, H):\n",
    "        # Apply the linear transformation\n",
    "   #     HW = self.linear(H)\n",
    "        \n",
    "   #     # Compute the new output with the added adjacency matrix term\n",
    "   #     output = torch.matmul(A_normalized, HW)\n",
    "        \n",
    "        # Apply ReLU activation function\n",
    "   #     return torch.relu(output)\n",
    "\n",
    "# GCN Layer (LeakyReLU)\n",
    "class GCNLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(GCNLayer, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim, bias=False)  # No bias in the linear layer\n",
    "        self.bias = nn.Parameter(torch.zeros(1, output_dim))  # Trainable bias matrix\n",
    "        # self.batch_norm = nn.BatchNorm1d(output_dim)  # Batch normalization\n",
    "        self.leaky_relu = nn.LeakyReLU(negative_slope=0.01)  # LeakyReLU activation\n",
    "\n",
    "    def forward(self, A_normalized, H):\n",
    "        # Apply the linear transformation\n",
    "        HW = self.linear(H)\n",
    "        \n",
    "        # Add the bias matrix\n",
    "        HW_plus_B = HW + self.bias\n",
    "        \n",
    "        # Apply batch normalization\n",
    "        # HW_plus_B = self.batch_norm(HW_plus_B)\n",
    "        \n",
    "        # Compute the new output with the bias matrix\n",
    "        output = torch.matmul(A_normalized, HW_plus_B)\n",
    "        \n",
    "        # Apply LeakyReLU activation function\n",
    "        return self.leaky_relu(output)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# GCN Model\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        self.gcn1 = GCNLayer(input_dim, hidden_dim)\n",
    "        self.gcn2 = GCNLayer(hidden_dim, hidden_dim)\n",
    "        self.gcn3 = GCNLayer(hidden_dim, hidden_dim)\n",
    "        self.gcn4 = GCNLayer(hidden_dim, hidden_dim)\n",
    "        # self.gcn5 = GCNLayer(hidden_dim_4, hidden_dim_5)\n",
    "        # self.gcn6 = GCNLayer(hidden_dim_5, hidden_dim_6)\n",
    "        # self.gcn7 = GCNLayer(hidden_dim_6, hidden_dim_7)\n",
    "        # self.gcn8 = GCNLayer(hidden_dim_7, hidden_dim_8)\n",
    "        # self.fc = nn.Linear(hidden_dim_8, output_dim)\n",
    "\n",
    "    def forward(self, A_normalized, X, num_atoms):\n",
    "        H = self.gcn1(A_normalized, X)\n",
    "        # print(f\"After GCN1: {H.shape}\")\n",
    "        \n",
    "        H = self.gcn2(A_normalized, H)\n",
    "        # print(f\"After GCN2: {H.shape}\")\n",
    "        \n",
    "        H = self.gcn3(A_normalized, H)\n",
    "        # print(f\"After GCN3: {H.shape}\")\n",
    "        \n",
    "        H = self.gcn4(A_normalized, H)\n",
    "        # print(f\"After GCN4: {H.shape}\")\n",
    "        \n",
    "        # Pooling over only valid atoms\n",
    "        mask = torch.arange(H.size(1), device=H.device).expand(H.size(0), -1) < num_atoms.unsqueeze(1)\n",
    "        H_masked = H * mask.unsqueeze(2)  # Zero out invalid rows\n",
    "        embeddings = H_masked.sum(dim=1) / num_atoms.unsqueeze(1).float()\n",
    "\n",
    "        \n",
    "        # out = self.fc(H)\n",
    "        # print(f\"Final output: {out.shape}\")\n",
    "        \n",
    "        return embeddings\n",
    "\n",
    "def weighted_mse_loss(y_pred, y_true, low_energy_weight=10.0):\n",
    "    \"\"\"\n",
    "    Custom weighted MSE loss that applies higher weights to the low-energy region.\n",
    "    \"\"\"\n",
    "   # Compute weights inversely proportional to true values\n",
    "    weights = torch.where(y_true < 0.5, low_energy_weight, 1.0)  # Weight low-energy (<0.5) more heavily\n",
    "    loss = weights * (y_pred - y_true) ** 2\n",
    "    return loss.mean()\n",
    "    \n",
    "# def weighted_mse_loss(y_pred, y_true, low_energy_weight=10.0):\n",
    "   #  weights = torch.where(y_true < 0.8, low_energy_weight, 1.0)\n",
    "   #  mse_loss = weights * (y_pred - y_true) ** 2\n",
    "   #  negative_penalty = torch.sum(torch.relu(-y_pred))  # Penalize negative predictions\n",
    "   #  return mse_loss.mean() + 0.1 * negative_penalty  # Add penalty term with weight\n",
    "\n",
    "\n",
    "# Main Training and Validation Function\n",
    "def train_and_validate(gcn_model, mlp_model, train_loader, val_loader, optimizer, epochs, low_energy_weight=10.0):\n",
    "    train_mse_list = []\n",
    "    val_mse_list = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        gcn_model.train()\n",
    "        mlp_model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for A, X, y, num_atoms in train_loader:\n",
    "            A, X, y, num_atoms = A.to(device), X.to(device), y.to(device), num_atoms.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # GCN Embedding\n",
    "            embeddings = gcn_model(A, X, num_atoms)\n",
    "            \n",
    "            # Prediction from MLP\n",
    "            y_pred = mlp_model(embeddings)\n",
    "            \n",
    "            # Loss and Backpropagation\n",
    "            loss = weighted_mse_loss(y_pred.squeeze(), y, low_energy_weight)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_mse_list.append(train_loss / len(train_loader))\n",
    "\n",
    "        # Validation Phase\n",
    "        gcn_model.eval()\n",
    "        mlp_model.eval()\n",
    "        val_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for A, X, y, num_atoms in val_loader:\n",
    "                A, X, y, num_atoms = A.to(device), X.to(device), y.to(device), num_atoms.to(device)\n",
    "                embeddings = gcn_model(A, X, num_atoms)\n",
    "                y_pred = mlp_model(embeddings)\n",
    "                loss = weighted_mse_loss(y_pred.squeeze(), y, low_energy_weight)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_mse_list.append(val_loss / len(val_loader))\n",
    "        scheduler.step(val_loss / len(val_loader))  # Update scheduler\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss / len(train_loader):.4f}, \"\n",
    "              f\"Val Loss: {val_loss / len(val_loader):.4f}\")\n",
    "\n",
    "    return train_mse_list, val_mse_list\n",
    "\n",
    "\n",
    "# Test set evaluation   \n",
    "def evaluate_test_set(model, test_loader):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for A, X, y, num_atoms in test_loader:\n",
    "            A, X, y, num_atoms = A.to(device), X.to(device), y.to(device), num_atoms.to(device)\n",
    "            predictions = model(A, X, num_atoms).squeeze()\n",
    "            y_true.extend(y.view(-1).cpu().numpy())  # Move to CPU for numpy conversion\n",
    "            y_pred.extend(predictions.view(-1).cpu().numpy())  # Move to CPU for numpy conversion\n",
    "\n",
    "    return np.array(y_true), np.array(y_pred)\n",
    "\n",
    "# Main\n",
    "if __name__ == \"__main__\":\n",
    "    # Hyperparameters\n",
    "    input_dim = 61  # Feature size \n",
    "    hidden_dim_gcn = 100\n",
    "    hidden_dim_mlp = 300\n",
    "    output_dim = 1\n",
    "    batch_size = 15\n",
    "    epochs = 1\n",
    "    learning_rate = 0.0001\n",
    "    low_energy_weight =1.0\n",
    "    \n",
    "    # Dataset\n",
    "    train_dataset = MolecularDataset(\"/home/george/TADF/gcn/training_set.csv\", target_col=\"ST_split\")\n",
    "    val_dataset = MolecularDataset(\"/home/george/TADF/gcn/validation_set.csv\", target_col=\"ST_split\")\n",
    "    test_dataset = MolecularDataset(\"/home/george/TADF/gcn/testing_set.csv\", target_col=\"ST_split\")\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Model, Loss, Optimizer, and Scheduler\n",
    "    gcn_model = GCN(input_dim, hidden_dim_gcn).to(device)\n",
    "    mlp_model = MLP(hidden_dim_gcn, hidden_dim_mlp, output_dim).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(list(gcn_model.parameters()) + list(mlp_model.parameters()), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=True)\n",
    "\n",
    "\n",
    "    # Train and validate\n",
    "    train_mse, val_mse = train_and_validate(gcn_model, mlp_model, train_loader, val_loader, optimizer, epochs, low_energy_weight)\n",
    "    \n",
    "\n",
    "    # Save MSE values to CSV\n",
    "    mse_df = pd.DataFrame({\n",
    "        \"Epoch\": list(range(1, epochs + 1)),\n",
    "        \"Train_MSE\": train_mse,\n",
    "        \"Validation_MSE\": val_mse\n",
    "    })\n",
    "    mse_csv_path = \"mse_train_validation_gcn_nn.csv\"\n",
    "    mse_df.to_csv(mse_csv_path, index=False)\n",
    "    print(f\"MSE values saved to '{mse_csv_path}'\")\n",
    "    \n",
    "    # Test set evaluation and R-squared\n",
    "    y_true, y_pred = evaluate_test_set(model, test_loader)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "    # Save the trained model\n",
    "    torch.save(model.state_dict(), \"gcn_nn.pth\")\n",
    "    print(\"Model saved to 'gcn_nn.pth'\")\n",
    "    \n",
    "    # Save MSE values and plots\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(1, epochs + 1), train_mse, label=\"Train MSE\")\n",
    "    plt.plot(range(1, epochs + 1), val_mse, label=\"Validation MSE\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Mean Squared Error (eV$^{2}$)\")\n",
    "    plt.title(\"Train and Validation MSE Over Epochs\")\n",
    "    plt.legend()\n",
    "    plt.savefig(\"train_val_mse_gcn_nn.png\")\n",
    "    print(\"Train and Validation MSE plot saved as 'train_val_mse_gcn_nn.png'\")\n",
    "\n",
    "    # Scatter plot for test set predictions\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.scatter(y_true, y_pred, alpha=0.7, label=\"Predicted vs Actual\")\n",
    "    plt.plot([min(y_true), max(y_true)], [min(y_true), max(y_true)], color=\"red\", linestyle=\"--\", label=\"Diagonal\")\n",
    "    plt.xlabel(\"Actual Values (eV)\")\n",
    "    plt.ylabel(\"Predicted Values (eV)\")\n",
    "    plt.title(\"Testing Set Predictions\")\n",
    "    #plt.legend()\n",
    "    plt.text(0.05, 0.95, f\"$R^2$: {r2:.2f}\", transform=plt.gca().transAxes, fontsize=12, verticalalignment='top')\n",
    "    plt.savefig(\"test_predictions_gcn_nn.png\")\n",
    "    print(\"Test set predictions plot saved as 'test_predictions_gcn_nn.png'\")\n",
    "\n",
    "\n",
    "    # Save MSE values to CSV\n",
    "    mse_df = pd.DataFrame({\"Epoch\": list(range(1, epochs + 1)), \"MSE\": mse_list})\n",
    "    mse_csv_path = \"mse_over_epochs_gcn_nn.csv\"\n",
    "    mse_df.to_csv(mse_csv_path, index=False)\n",
    "    print(f\"MSE values saved to '{mse_csv_path}'\")\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
